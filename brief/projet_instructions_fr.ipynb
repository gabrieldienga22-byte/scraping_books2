{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a65c22e",
   "metadata": {},
   "source": [
    "# üìö Projet ‚Äì Books to Scrape\n",
    "\n",
    "## ‚è±Ô∏è Temps Estim√© : **300 minutes (5 heures)**\n",
    "\n",
    "Ce projet vous guidera dans la cr√©ation d‚Äôun **web scraper** pour le site [Books to Scrape](https://books.toscrape.com/). Il est con√ßu comme un exercice pratique pour s‚Äôentra√Æner avec **Python, requests, Pandas et l‚Äôanalyse de donn√©es**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04fcdfc2",
   "metadata": {},
   "source": [
    "## üéØ Contexte\n",
    "\n",
    "L‚Äô√©quipe marketing d‚Äôune librairie en ligne souhaite mieux comprendre son catalogue. Elle veut collecter des informations sur tous les livres, analyser les cat√©gories, les prix, les notes et la disponibilit√© en stock.\n",
    "\n",
    "En tant que data scientist, votre mission est de **scraper le site web** et de livrer des jeux de donn√©es structur√©s ainsi que des premiers insights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b74b63",
   "metadata": {},
   "source": [
    "## ‚úÖ Objectifs\n",
    "\n",
    "1. Scraper le site [Books to Scrape](https://books.toscrape.com/)\n",
    "2. Extraire pour chaque livre :\n",
    "   - Titre\n",
    "   - Prix\n",
    "   - Disponibilit√© en stock\n",
    "   - Note\n",
    "   - URL du produit\n",
    "   - URL de l‚Äôimage\n",
    "   - UPC\n",
    "   - Cat√©gorie\n",
    "3. G√©rer la **pagination** sur toutes les pages\n",
    "4. Sauvegarder les r√©sultats dans **un CSV par cat√©gorie**\n",
    "5. T√©l√©charger les images des couvertures de livres dans des dossiers par cat√©gorie\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df80118e",
   "metadata": {},
   "source": [
    "## üì¶ Livrables\n",
    "\n",
    "- Fichiers CSV : `outputs/csv/category_<slug>.csv`\n",
    "- Images : `outputs/images/<category>/<upc>_<slug-title>.jpg`\n",
    "- Optionnel : Un notebook de nettotage et d‚Äôexploration qui analyse les prix, notes et stocks avec Pandas et Quelques visualisation √† r√©aliser avec les packages que vous pr√©f√©r√©."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0964718c",
   "metadata": {},
   "source": [
    "## üõ† √âtapes sugg√©r√©es\n",
    "\n",
    "1. Commencez par scraper **une page produit** et extraire les champs demand√©s\n",
    "2. √âtendez votre code √† **une cat√©gorie** (gestion de plusieurs pages)\n",
    "3. G√©n√©ralisez votre scraper √† **toutes les cat√©gories**\n",
    "4. Sauvegardez les r√©sultats dans des fichiers CSV\n",
    "5. √âtendez le scraper pour aussi **t√©l√©charger les images**\n",
    "6. (Optionnel) Explorez le dataset avec Pandas (prix moyen par cat√©gorie, distribution des notes, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695ffcdb",
   "metadata": {},
   "source": [
    "# üõ† √âtapes d√©taill√©es\n",
    "\n",
    "## üü¢ Phase 1 ‚Äì Construire pas √† pas (dans un seul script au d√©but)\n",
    "1. **R√©cup√©rer une page** ‚Üí utiliser `requests` pour t√©l√©charger le HTML.  \n",
    "2. **Extraire les titres** ‚Üí avec `Selector`, r√©cup√©rer les noms des livres sur la page d‚Äôaccueil.  \n",
    "3. **Extraire les d√©tails** ‚Üí ouvrir la page d‚Äôun livre et extraire :\n",
    "   - titre  \n",
    "   - prix  \n",
    "   - disponibilit√© en stock  \n",
    "   - note (rating)  \n",
    "   - UPC  \n",
    "   - **URL de l‚Äôimage** (utiliser `.css(\"img::attr(src)\")` + `urljoin` pour obtenir le lien absolu)  \n",
    "4. **G√©rer une cat√©gorie** ‚Üí collecter toutes les URLs de livres dans une page de cat√©gorie.  \n",
    "5. **G√©rer plusieurs pages** ‚Üí suivre le bouton `\"li.next a\"` jusqu‚Äô√† ce qu‚Äôil n‚Äôy en ait plus.  \n",
    "6. **Sauvegarder les r√©sultats** ‚Üí √©crire les r√©sultats dans un fichier CSV (`outputs/csv/category_<name>.csv`).  \n",
    "7. **T√©l√©charger les images** ‚Üí utiliser l‚ÄôURL de l‚Äôimage pour la t√©l√©charger avec `requests` :\n",
    "   - Chemin : `outputs/images/<categorie>/<upc>_<slug-title>.jpg`  \n",
    "8. **G√©rer plusieurs cat√©gories** ‚Üí boucler sur toutes les cat√©gories depuis la page d‚Äôaccueil.  \n",
    "\n",
    "\n",
    "## üü° Phase 2 ‚Äì Organiser le projet\n",
    "Une fois que le code fonctionne, s√©parer en plusieurs fichiers :  \n",
    "- `parsers.py` ‚Üí fonctions de scraping (par ex. `parse_list_page`, `parse_product_page`, `get_category_links`)  \n",
    "- `utils.py` ‚Üí fonctions utilitaires (par ex. `write_csv`, `download_file`, `ensure_dir`)  \n",
    "- `settings.py` ‚Üí constantes (`BASE_URL`, `HEADERS`, `DEFAULT_DELAY`, `TIMEOUT`)  \n",
    "- `scrape.py` ‚Üí script principal (seulement `main()` + argparse), appelle les fonctions des autres fichiers  \n",
    "\n",
    "\n",
    "## üîµ Phase 3 ‚Äì Automatiser avec la ligne de commande (CLI)\n",
    "Ajouter des **options argparse** dans `scrape.py` :  \n",
    "- `--categories Travel,Poetry` ‚Üí scraper seulement certaines cat√©gories  \n",
    "- `--max-pages 1` ‚Üí limiter le scraping pour des tests rapides  \n",
    "- `--delay 1` ‚Üí ajouter un d√©lai entre les requ√™tes  \n",
    "- `--outdir outputs` ‚Üí changer le dossier de sortie  \n",
    "\n",
    "Exemple d‚Äôutilisation :\n",
    "```bash\n",
    "python scrape.py --categories Travel --max-pages 1\n"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-22T15:00:15.202194Z",
     "start_time": "2025-10-22T15:00:15.198617Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import requests\n",
    "from parsel import Selector\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "\n",
    "# URL de base\n",
    "url = \"https://books.toscrape.com/\"\n"
   ],
   "id": "3f162b3e3f89c5f5",
   "outputs": [],
   "execution_count": 64
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-22T15:00:18.060419Z",
     "start_time": "2025-10-22T15:00:17.258259Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Faire la requ√™te\n",
    "r = requests.get(url)\n",
    "print(\"Status code:\", r.status_code)\n",
    "\n",
    "# Cr√©er le s√©lecteur\n",
    "response = Selector(text=r.text)\n"
   ],
   "id": "ac32f0c9ccda4510",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status code: 200\n"
     ]
    }
   ],
   "execution_count": 65
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-22T15:00:20.053839Z",
     "start_time": "2025-10-22T15:00:20.048527Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Tester sur un livre\n",
    "book = response.css('article.product_pod')[0]\n",
    "\n",
    "# Titre\n",
    "title = book.css('h3 a::attr(title)').get()\n",
    "print(\"Titre:\", title)\n",
    "\n",
    "# Prix\n",
    "price = book.css('p.price_color::text').get()\n",
    "print(\"Prix:\", price)\n",
    "\n",
    "# Note (rating)\n",
    "rating_class = book.css('p.star-rating::attr(class)').get()\n",
    "print(\"Rating class:\", rating_class)\n",
    "\n",
    "# Disponibilit√© (nettoy√©e)\n",
    "availability = book.css('p.instock.availability::text').getall()\n",
    "availability_clean = [a.strip() for a in availability if a.strip()]\n",
    "print(\"Disponibilit√©:\", availability_clean)\n",
    "\n",
    "# URL du livre\n",
    "book_url = book.css('h3 a::attr(href)').get()\n",
    "print(\"URL:\", book_url)\n",
    "\n",
    "# URL de l'image\n",
    "image_url = book.css('div.image_container img::attr(src)').get()\n",
    "print(\"Image:\", image_url)\n"
   ],
   "id": "41bdd21c09a8b6fc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Titre: A Light in the Attic\n",
      "Prix: √Ç¬£51.77\n",
      "Rating class: star-rating Three\n",
      "Disponibilit√©: ['In stock']\n",
      "URL: catalogue/a-light-in-the-attic_1000/index.html\n",
      "Image: media/cache/2c/da/2cdad67c44b002e7ead0cc35693c0e8b.jpg\n"
     ]
    }
   ],
   "execution_count": 66
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-22T15:05:19.949449Z",
     "start_time": "2025-10-22T15:05:19.945812Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_rating(rating_class):\n",
    "    \"\"\"Convertit la classe CSS en nombre d'√©toiles\"\"\"\n",
    "    ratings = {\n",
    "        'One': 1,\n",
    "        'Two': 2,\n",
    "        'Three': 3,\n",
    "        'Four': 4,\n",
    "        'Five': 5\n",
    "    }\n",
    "    rating_word = rating_class.split()[-1]\n",
    "    return ratings.get(rating_word, 0)\n",
    "\n",
    "# Test\n",
    "rating = get_rating('star-rating Three')\n",
    "print(\"Rating:\", rating, \"√©toiles\")\n"
   ],
   "id": "cd25b61ac80b4446",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rating: 3 √©toiles\n"
     ]
    }
   ],
   "execution_count": 67
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-22T15:05:31.500425Z",
     "start_time": "2025-10-22T15:05:31.490627Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# R√©cup√©rer tous les livres de la page\n",
    "books = response.css('article.product_pod')\n",
    "print(\"Nombre de livres sur la page:\", len(books))\n",
    "\n",
    "# Liste pour stocker les donn√©es\n",
    "data = []\n",
    "\n",
    "# Boucle sur chaque livre\n",
    "for book in books:\n",
    "    title = book.css('h3 a::attr(title)').get()\n",
    "    price = book.css('p.price_color::text').get()\n",
    "    rating_class = book.css('p.star-rating::attr(class)').get()\n",
    "    rating = get_rating(rating_class)\n",
    "    availability = book.css('p.instock.availability::text').getall()\n",
    "    in_stock = 'In stock' in ' '.join(availability)\n",
    "    book_url = book.css('h3 a::attr(href)').get()\n",
    "    image_url = book.css('div.image_container img::attr(src)').get()\n",
    "\n",
    "    data.append({\n",
    "        'title': title,\n",
    "        'price': price,\n",
    "        'rating': rating,\n",
    "        'in_stock': in_stock,\n",
    "        'url': book_url,\n",
    "        'image': image_url\n",
    "    })\n",
    "\n",
    "print(\"Livres extraits:\", len(data))\n"
   ],
   "id": "bc01c18f27cdfc36",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de livres sur la page: 20\n",
      "Livres extraits: 20\n"
     ]
    }
   ],
   "execution_count": 68
  },
  {
   "cell_type": "markdown",
   "id": "937efb57",
   "metadata": {},
   "source": [
    "## üü£ Phase 4 ‚Äì Documenter\n",
    "- Cr√©er un fichier `README.md` avec :\n",
    "  - L‚Äôobjectif du projet et son contexte  \n",
    "  - Comment installer les d√©pendances (`pip install -r requirements.txt`)  \n",
    "  - Comment ex√©cuter le scraper  \n",
    "  - Quelques exemples de commandes  \n",
    "\n",
    "\n",
    "## üî¥ Phase 5 ‚Äì Partager\n",
    "- Publier votre projet sur GitHub avec :\n",
    "  - Les fichiers de code (`scrape.py`, `parsers.py`, `utils.py`, `settings.py`)  \n",
    "  - Un exemple de `README.md`  \n",
    "  - Un dossier `outputs/` vide avec un fichier `.gitkeep` pour conserver la structure  \n",
    "\n",
    "\n",
    "## üü† Phase 6 ‚Äì Explorer les donn√©es\n",
    "Ouvrir un **notebook Jupyter** pour analyser les donn√©es extraites :\n",
    "\n",
    "1. **Charger un CSV** avec Pandas :\n",
    "   ```python\n",
    "   import pandas as pd\n",
    "   df = pd.read_csv(\"outputs/csv/category_travel.csv\")\n",
    "   df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884ba8f6",
   "metadata": {},
   "source": [
    "## üìù Crit√®res d‚Äô√©valuation\n",
    "\n",
    "- üí° Effort et compr√©hension :  \n",
    "  - Je valorise **vos propres essais** plus qu‚Äôun simple copier-coller depuis ChatGPT ou Internet.  \n",
    "  - M√™me un progr√®s partiel, des commentaires clairs dans le code, ou diff√©rentes tentatives montrent un v√©ritable apprentissage.  \n",
    "  - Vous devez √™tre capable d‚Äô**expliquer votre code** lors de la relecture ou de la pr√©sentation.  \n",
    "\n",
    "- üåü Points bonus pour la partie analyse de la donn√©e √† la fin avec un notebook et de la recherche sur itnernet de comment faire des visualisations pertinentes et cr√©ative dans le notebook (vous pouvez explorer la librarie **plotly express**).  \n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
